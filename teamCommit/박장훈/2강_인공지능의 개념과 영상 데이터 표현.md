# 2강 - 인공지능의 개념과 영상 데이터 표현

## 기계학습의 구성 요소
- 경험 사례
  - data
- 모델
  - 인공신경망
- 평가 기준
  - cost/lost function

## 가장 좋은 모델 f()는 어떻게 찾을 수 있을까?
- Annotation을 통한 학습으로 이루어짐
- min_f ||Label - f(data)||
  - min : 최적화
  - f : 모델

## 기계학습의 적용 예시
- 문제 정의:
  - 주어진 주식 변동 그래프를 통하여
  - 시간의 흐름과 주가와의 관계를 알고 싶음
    - 시간 -> Model f() -> 주가
    - 과거의 결과들 = Data

## 가장 좋은 모델 f()는 어떻게 찾을 수 있을까?
- Data
  - 시간의 흐름, 주가 테이블
  - 그래프로 표현 가능
- 모델
  - f(x) : 선형모델, 이차방정식, 뉴럴넷(인공신경망)
  - min_f ||y - f(x)||
  - 선형모델 f(x) = Wx + b
    - W, b : 학습 가능한 파라미터

## 현재 모델 f()가 얼마 만큼 틀렸는지 어떻게 측정할까?
- 평가(loss)
  - cost([W, b]) = 1/m * SUM(i = 1 to m)(f(x_i) - y_i)^2
  - f(x_i) : 예측값(모델의 output)
  - y_i : 목표값(데이터에 포함된 답)
- 파라미터를 특정 방향으로 변경하여 cost 값이 줄어드는 방향으로 변경

## 어떻게 하면 현재 모델 f()를 개선할 수 있을까?
- 최적화
  - Global minima
  - Local minima
  - 한 시작점에서 cost가 줄어드는 방향으로 이동하는것 
    - Gradient를 추정하는 과정
  - Gradient
    - cost([W, b]) = 1/m * SUM(i = 1 to m)(W*x_i - y_i)^2 일때
    - Grad(W) = dcost/dW = 2/m * SUM(i=1 to m)(W*x_i - y_i)x_i
  - W_next = W - a Grad(W)
    - a : Learning rate, step size
    - Gradient Decent, 경사 하강법
      - Gradient는 (정의 때문에) 상승하는 방향을 가리킴
        - Grad(f(x, y)) = (f_x, f_y)
      - Gradient Decent란 Gradient가 가리키는 방향의 반대 방향으로 이동하는 방식
  - a 는 직접 설정해줘야함
    - 너무 크면 정확한 값을 찾기가 힘듦
    - 너무 작으면 결국 찾겠지만 너무 오래걸림(에너지 낭비)
    - (응용수학) learning rate a를 변화시키면서 최소값을 찾는 steepest decent라는 방법도 있음 
- 모델 -> 평가 -> 최적화 -> 모델 -> ...
  - 최적화 과정
- gradient를 구할때 m개의 데이터를 모두 사용해서 계산하면 너무 비쌈
  - 전체 데이터가 100개면 10개 단위로 쪼갬
  - 쪼개진 10개의 데이터를 mini batch라고 함
  - 한번 gradient를 구할때 100개가 아닌 10개의 데이터만 사용하여 빠르게 구함
  - mini batch gradient decent/stochastic gradient decent
  - 10번의 mini batch 반복을 하여 전체 데이터셋을 한번 다 보는 것을 1 epoch라고 함
    - epoch : train dataset 전체를 한번 다 이용하면 1씩 상승

## 이렇게 학습 할때 발생하는 잠재적 문제
- 본 적이 있는 것 / 본 적이 없는 것
  - 본 적 없는 부분에서 에러가 크게 나올 수 있음
  - 일반화 문제

## 기계학습이란 무엇인가?
- input x -> function f -> output y
- function이 주어짐
  - input을 넣고 output이 나옴
  - 계산/추론 (computation/inference)
  - 함수를 주고, Output이 나오도록 계산
    - 예시) F = ma, V = IR
- input과 output이 주어짐
  - function f를 찾음
  - 최적화 (Optimization)
  - 한 세트의(input, output)을 주고, 관계를 가장 잘 설명하는 f를 찾는 것
- 본적이 없는 input이 주어짐
  - 의미 있는 output이 나옴
  - 일반화 (Generalization)

## 어떻게 하면 이 문제를 해결할 수 있을까
- 충분히 많은 양의 데이터 사용
- 모델의 복잡도 줄이기
  - 복잡도가 너무 높으면 예상 밖의 output이 나올 수 있음
- 가중치의 regularization 적용하기
  - 레이어 W 범위 [-c, c]
  - 로스 펑션 L 에러를 줄이는.. L + p*||W||^2 뒷부분 추가
  - 정답을 맞추면서도 W의 크기가 작은 모델 파라미터를 찾으려고 함
- 드롭아웃(drop out)
  - 노이즈나 시련을 주는 방법
  - A = W*X
    - W 웨이트
    - X 액티베이션 인풋
    - A 다음 액티베이션
  - 다음 레이어에 액티베이션 주기 전에 강제로 0으로 바꿈
  - 그럼에도 불구하고 task가 잘 수행할 수 있게 하는 것
  - Input에 노이즈를 줄 수도 있고 weight에 노이즈를 줄 수도 있음

## Deep Neural Network
- Univeral approximator theorem
  - 모델이 충분히 크다면 input과 output의 관계를 모두 학습할 수 있음
  - f(x) = s(W'' * ... * s(W' * s(Wx + b) + b') + ... + b'')

## 기계학습 패러다임의 변화
- (기존) Machine Learning
  - 입력 -> 특징 추출 -> 분류 -> 출력
    - 데이터를 잔뜩 모아놓음
    - 사람이 데이터를 보면서 어떤 특징이 좋은 특징일까를 고름
      - Q) 좋은 특징 (feature)는 무엇인가? 출력과의 관계를 잘 설명 할 수 있는 것
        - ex) 주가의 feature = 시간의 흐름(좋은 특징), 동전의 무게(나쁜 특징) 등...
    - 사람들이 이런 특징들을 잘 만들었다고 가정
    - 분류는 간단한 모델을 사용
    - 괜찮은 결과 출력
  - 세상은 더 복잡해서 많은 부분을 놓치고 있었고 고려해야했음
- Deep Learning
  - 입력 -> 특징 추출 & 분류 -> 출력
  - 입력에서 출력까지를 미분 가능한 뉴럴 네트워크 형태로
  - End2End (Universal approximator)

---
- 뉴럴 네트워크란?